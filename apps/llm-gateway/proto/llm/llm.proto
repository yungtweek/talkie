syntax = "proto3";

package llm.v1;

option go_package = "github.com/yungtweek/talkie/apps/llm-gateway/gen/llm;llm";

// 공통 메타데이터 (로그/추적용)
message LlmMetadata {
  string request_id = 1; // 워커 쪽에서 생성하는 요청 ID
  string trace_id   = 2; // 분산 트레이싱용 (선택)
  string session_id = 3; // 유저 세션 / 대화 ID
  string user_id    = 4; // 유저 식별자 (익명 ID 가능)
}

// ChatCompletion 요청
message ChatCompletionRequest {
  LlmMetadata meta = 1;

  string model         = 2; // 예: "qwen2.5-7b-instruct"
  string system_prompt = 3; // 시스템 역할 프롬프트
  string user_prompt   = 4; // 유저 실제 질문
  string context       = 5; // RAG 컨텍스트 텍스트 (없으면 빈 문자열)

  double temperature = 6; // 0.0 ~ 2.0
  int32  max_tokens  = 7; // 생성 최대 토큰 수
  double top_p       = 8; // nucleus sampling
}

// ChatCompletion 응답
message ChatCompletionResponse {
  string output_text   = 1; // 최종 생성 텍스트
  string finish_reason = 2; // 예: "stop", "length", "error" 등

  int32 prompt_tokens     = 3;
  int32 completion_tokens = 4;
  int32 total_tokens      = 5;

  int64 latency_ms = 6; // gateway 기준 end-start ms
}

message ChatCompletionChunkResponse {
  // OpenAI Responses-style event type
  // e.g. "response.output_text.delta", "response.completed", "response.error"
  string type = 1;

  // Streaming payload
  string text = 2; // delta text for *.delta events

  // Completion metadata (set on final/completed event)
  string finish_reason = 3;
  int32  index = 4;

  int32  prompt_tokens = 5;
  int32  completion_tokens = 6;
  int32  total_tokens = 7;

  int64  latency_ms = 8; // gateway 기준 end-start ms
}

// LLM Gateway gRPC 서비스 정의
service LlmService {
  // 단일 요청/응답 ChatCompletion (비스트리밍)
  rpc ChatCompletion(ChatCompletionRequest) returns (ChatCompletionResponse);
  rpc ChatCompletionStream(ChatCompletionRequest) returns (stream ChatCompletionChunkResponse);
}